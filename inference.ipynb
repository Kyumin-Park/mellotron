{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.io.wavfile import write\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from hparams import create_hparams\n",
    "from model import Tacotron2, load_model\n",
    "from waveglow.denoiser import Denoiser\n",
    "from layers import TacotronSTFT\n",
    "from data_utils import TextMelLoader, TextMelCollate\n",
    "from text import cmudict, text_to_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panner(signal, angle):\n",
    "    angle = np.radians(angle)\n",
    "    left = np.sqrt(2)/2.0 * (np.cos(angle) - np.sin(angle)) * signal\n",
    "    right = np.sqrt(2)/2.0 * (np.cos(angle) + np.sin(angle)) * signal\n",
    "    return np.dstack((left, right))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mel_f0_alignment(mel_source, mel_outputs_postnet, f0s, alignments, figsize=(16, 16)):\n",
    "    fig, axes = plt.subplots(4, 1, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    axes[0].imshow(mel_source, aspect='auto', origin='lower', interpolation='none')\n",
    "    axes[1].imshow(mel_outputs_postnet, aspect='auto', origin='lower', interpolation='none')\n",
    "    axes[2].scatter(range(len(f0s)), f0s, alpha=0.5, color='red', marker='.', s=1)\n",
    "    axes[2].set_xlim(0, len(f0s))\n",
    "    axes[3].imshow(alignments, aspect='auto', origin='lower', interpolation='none')\n",
    "    axes[0].set_title(\"Source Mel\")\n",
    "    axes[1].set_title(\"Predicted Mel\")\n",
    "    axes[2].set_title(\"Source pitch contour\")\n",
    "    axes[3].set_title(\"Source rhythm\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mel(path):\n",
    "    audio, sampling_rate = librosa.core.load(path, sr=hparams.sampling_rate)\n",
    "    audio = torch.from_numpy(audio)\n",
    "    if sampling_rate != hparams.sampling_rate:\n",
    "        raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    melspec = melspec.cuda()\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = create_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"여기서 체크포인트 파일 바꿈\"\"\"\n",
    "checkpoint_path = \"models/checkpoint_74500\"\n",
    "mellotron = load_model(hparams).cuda().eval()\n",
    "mellotron.load_state_dict(torch.load(checkpoint_path)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waveglow_path = 'models/waveglow_256channels_universal_v4.pt'\n",
    "waveglow = torch.load(waveglow_path)['model'].cuda().eval()\n",
    "denoiser = Denoiser(waveglow).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arpabet_dict = cmudict.CMUDict('data/cmu_dictionary')\n",
    "\"\"\"여기서 audio_paths 파일로 정해서 하면됨\"\"\"\n",
    "# audio_paths = 'data/examples_filelist.txt'\n",
    "audio_paths = 'sample/filelist_vctk_val.txt'\n",
    "# audio_paths = 'sample/filelist_nonparallel.txt'\n",
    "dataloader = TextMelLoader(audio_paths, hparams)\n",
    "datacollate = TextMelCollate(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "        \"the party has never fully recovered.\",\n",
    "        \"we also need a small plastic snake and a big toy frog for the kids.\",\n",
    "        # \"this is a librivox recording.\",\n",
    "        # \"matthew cuthbert is surprised\",\n",
    "        \"ambitious hopes, which had seemed to be extinguished, revived in his bosom.\",\n",
    "        # \"after a pause bechamel went back to the dining room.\",\n",
    "        # \"in the aftermath of this storm, we were thrown back to the east. away went any hope of\",\n",
    "        \"when we first met here we were younger than our girls are now.\",\n",
    "        # \"you must know said margolotte when they were all seated together on the broad window seat that my husband foolishly gave away all the powder of life he first made to old mombi the witch who used to live in the country of the gillikins to the north of here.\",\n",
    "        \"oh my god, he's lost it. he's totally lost it.\",\n",
    "        \"Now it was finished - that is to say the design - she must stitch it together .\",\n",
    "        # \"Advanced text to speech models such as Fast Speech can synthesize speech significantly faster than previous auto regressive models with comparable quality. The training of Fast Speech model relies on an auto regressive teacher model for duration prediction and knowledge distillation, which can ease the one to many mapping problem in T T S. However, Fast Speech has several disadvantages, 1, the teacher student distillation pipeline is complicated, 2, the duration extracted from the teacher model is not accurate enough, and the target mel spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality.\",\n",
    "        # \"Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\",\n",
    "        # \"in being comparatively modern.\",\n",
    "        # \"For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands, by a similar process\",\n",
    "        # \"produced the block books, which were the immediate predecessors of the true printed book,\",\n",
    "        # \"the invention of movable metal letters in the middle of the fifteenth century may justly be considered as the invention of the art of printing.\",\n",
    "        \"And it is worth mention in passing that, as an example of fine typography,\",\n",
    "        # \"the earliest book printed with movable types, the Gutenberg, or \\\"forty-two line Bible\\\" of about 1455,\",\n",
    "        # \"has never been surpassed.\",\n",
    "        \"Printing, then, for our purpose, may be considered as the art of making books by means of movable types.\",\n",
    "        # \"Now, as all books not primarily intended as picture-books consist principally of types composed to form letterpress,\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"여기서 file_idx 고쳐서 audio path 다른거 뽑고 text 고르면됨\"\"\"\n",
    "file_idx = 0\n",
    "audio_path, text, sid = dataloader.audiopaths_and_text[file_idx]\n",
    "# text = random.sample(sentences, 1)[0]\n",
    "print(audio_path, text)\n",
    "\n",
    "# get audio path, encoded text, pitch contour and mel for gst\n",
    "text_encoded = torch.LongTensor(text_to_sequence(text, hparams.text_cleaners))[None, :].cuda()\n",
    "pitch_contour = dataloader[file_idx][3][None].cuda()\n",
    "mel = load_mel(audio_path)\n",
    "print(text_encoded.size(), pitch_contour.size(), mel.size())\n",
    "\n",
    "# load source data to obtain rhythm using tacotron 2 as a forced aligner\n",
    "# x, y = mellotron.parse_batch(datacollate([dataloader[file_idx]]))\n",
    "x, y = mellotron.parse_batch(datacollate([dataloader.get_data((audio_path, text, sid))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'text_padded: {x[0].size()}')\n",
    "print(f'input_lengths: {x[1].size()}')\n",
    "print(f'mel_padded: {x[2].size()}')\n",
    "print(f'max_len: {x[3]}')\n",
    "print(f'output_lengths: {x[4].size()}')\n",
    "print(f'speaker_ids: {x[5].size()}')\n",
    "print(f'f0_padded: {x[6].size()}')\n",
    "print(f'mel_padded: {y[0].size()}')\n",
    "print(f'gate_padded: {y[1].size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_path, rate=hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Speakers Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids = TextMelLoader(\"filelists/libritts_train_clean_100_audiopath_text_sid_shorterthan10s_atleast5min_train_filelist.txt\", hparams).speaker_ids\n",
    "speakers = pd.read_csv('filelists/libritts_speakerinfo.txt', engine='python',header=None, comment=';', sep=' *\\| *', \n",
    "                       names=['ID', 'SEX', 'SUBSET', 'MINUTES', 'NAME'])\n",
    "speakers['MELLOTRON_ID'] = speakers['ID'].apply(lambda x: speaker_ids[x] if x in speaker_ids else -1)\n",
    "female_speakers = cycle(\n",
    "    speakers.query(\"SEX == 'F' and MINUTES > 20 and MELLOTRON_ID >= 0\")['MELLOTRON_ID'].sample(frac=1).tolist())\n",
    "male_speakers = cycle(\n",
    "    speakers.query(\"SEX == 'M' and MINUTES > 20 and MELLOTRON_ID >= 0\")['MELLOTRON_ID'].sample(frac=1).tolist())\n",
    "print(next(female_speakers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vctk-speaker-info.txt') as f:\n",
    "    speaker_lines = f.readlines()[1:]\n",
    "\n",
    "speakers = [(l.split()[0].strip(), l.split()[2].strip()) for l in speaker_lines]\n",
    "male_speakers = [(s[0], i) for i, s in enumerate(speakers) if s[1] == 'M']\n",
    "female_speakers = [(s[0], i) for i, s in enumerate(speakers) if s[1] == 'F']\n",
    "speakers_dict = dict(male_speakers + female_speakers)\n",
    "print(speakers_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer (Rhythm and Pitch Contour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # get rhythm (alignment map) using tacotron 2\n",
    "    mel_outputs, mel_outputs_postnet, gate_outputs, rhythm = mellotron.forward(x)\n",
    "    rhythm = rhythm.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"여기서 위에 보고 스피커 id 바꿈\"\"\"\n",
    "speaker_id = female_speakers[0][1] if np.random.randint(2) else male_speakers[0][1]\n",
    "speaker_id = torch.LongTensor([speaker_id]).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    mel_outputs, mel_outputs_postnet, gate_outputs, _ = mellotron.inference_noattention(\n",
    "        (text_encoded, mel, speaker_id, pitch_contour, rhythm))\n",
    "\n",
    "plot_mel_f0_alignment(x[2].data.cpu().numpy()[0],\n",
    "                      mel_outputs_postnet.data.cpu().numpy()[0],\n",
    "                      pitch_contour.data.cpu().numpy()[0, 0],\n",
    "                      rhythm.data.cpu().numpy()[:, 0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    audio = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.8), 0.01)[:, 0]\n",
    "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
